{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30619103",
   "metadata": {},
   "source": [
    "### Задача 1 \n",
    "\n",
    "Дописать функцию с выбором действий на основе ВДГ-действий `get_action()` и\n",
    "дописать симуляцию бандита с выбором действий по стратегии с выборкой Томпсона (+0,5 буквы).\n",
    "\n",
    "См. файл `2.0 RL_MAB_on.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ddc3-76db-4c1d-a486-5eb25bca9aa9",
   "metadata": {},
   "source": [
    "### Задача 2\n",
    "\n",
    "Реализовать точный метод оценки стратегии через решение СЛАУ (+0,5 буквы).\n",
    "\n",
    "См. файл `2.1.1 RL_DP_PolicyEval_on.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae7c3e",
   "metadata": {},
   "source": [
    "### Задача 3\n",
    "\n",
    "Дописать алгоритм итерации по ценности для поиска оптимальной стратегии (+0,5 буквы).\n",
    "\n",
    "См. файл `2.1.2 RL_DP_OptimalPolicy_on.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b3710",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "\n",
    "Реализовать метод Монте-Карло для поиска оптимальной стратегии с исследовательскими стартами, сравнить $\n",
    "varepsilon$-жадную стратегию и жадную но с исследовательскими стартами на примере среды `FrozenLake`  (+0,5 буквы).\n",
    "\n",
    "См. файл: `2.2.1 RL_MC_OptimalPolicy_on.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250575e",
   "metadata": {},
   "source": [
    "### Задача 5\n",
    "\n",
    "Реализовать обучение табличного агента на основе метода ExpectedSARSA, как метода обучения с единой стратегией с обобщённой итерацией по $\\varepsilon$-жадным стратегиям  (+0,5 буквы).\n",
    "\n",
    "См. файл: `2.3.2 RL_TD_ExpectedSARSA_on.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdf9d5-eeb2-4793-a5f8-8f24264c7e69",
   "metadata": {},
   "source": [
    "### Задача 6\n",
    "\n",
    "Обучить агента одним из изученных выше методов взаимодействию со средой \n",
    "\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "Базовое описание среды можно найти по ссылке:\n",
    "\n",
    "    https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
    "\n",
    "Цель в обучении агента до такого уровня, чтобы при прогоне на 1000 эпизодах средний доход превысил -105  (+0,5 буквы).\n",
    "\n",
    "Если средний доход превысит -85, то +1 буква.\n",
    "\n",
    "См. файл `2.3.4 RL_TD_N_step_SARSA_on.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05929c0-1519-4d14-85cb-3a8e7c846872",
   "metadata": {},
   "source": [
    "### Задача 7 (+0.5 буквы)\n",
    "\n",
    "См. файл `2.4.2 RL_DoubleDQN_PyTorch_on.ipynb`\n",
    "\n",
    "Рассмотрим задачу о машинке и холме MountainCar. В этой задаче вознаграждения формируются так: -1 за каждый временной шаг. Ранее мы наблюдали, что первая успешная попытка добраться до вершины холма может занять у обучаемого агента значительное время подобрать. Этот же эффект будет наблюдаться и при применении DQN. Первый успех может быть лишь через 500-600 эпизодов обучения и больше. Для ускорения надо изменить способ формирования вознаграждения (англ. Reward Shaping).\n",
    "\n",
    "Задача заключается в том, чтобы подобрать способ формирования вознаграждения, при котором первое успешное достижение вершины холма возникает ранее 250 эпизода.\n",
    "\n",
    "Также эту систему вознаграждений можно применить и при обучении методом REINFORCE в задаче о машинке и холме MountainCar.\n",
    "\n",
    "См. файл `2.5 REINFORCE_PyTorch on.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5adf0b-1388-41c7-afbb-d74802738410",
   "metadata": {},
   "source": [
    "### Задача 8 (+1 буква)\n",
    "\n",
    "См. файл 2.4.2 RL_DoubleDQN_PyTorch_on.ipynb и лекции \"Слайд 11\". Реализовать для  модификацию метода DQN с приоритетным воспроизведением опыта. Сравнить обучение методом DQN с/без этой модификации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a761aab-1507-46bd-aa86-f38511048067",
   "metadata": {},
   "source": [
    "### Задача 9 \n",
    "\n",
    "Обучить агента в среде, которая не рассматривалась на лекциях. (+ 0.5 или +1 буква в зависимости от сложности среды)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa452c-fbf6-44bb-904f-39d8c47acfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
